{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\anaconda3\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import YouTubeVideo\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(driver,id):\n",
    "    '''\n",
    "    Sets up a function for scraping data based on a given driver\n",
    "\n",
    "    args: driver (defines which driver to scrape from)\n",
    "    '''\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    table_node = soup.find(id=id)\n",
    "\n",
    "    columns_html = table_node.thead.find_all('th')\n",
    "        # Extract the text\n",
    "    columns = []\n",
    "    for col in columns_html:\n",
    "        columns.append(col.text)\n",
    "    columns = columns[7:]\n",
    "\n",
    "    rows_list = table_node.tbody.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    fejl = []\n",
    "    for row_node in rows_list:\n",
    "        # if (rows_list.index(row_node) % 25 == 0):\n",
    "        try:\n",
    "            row = []\n",
    "            for child in row_node.children:\n",
    "                row.append(child.text)\n",
    "            data.append(row)\n",
    "        except:\n",
    "            print(rows_list.index(row_node))\n",
    "            fejl.append(rows_list.index(row_node)) # Column appear every 25th row\n",
    "    \n",
    "    return columns, data\n",
    "\n",
    "def clean(columns, data,period):\n",
    "    df_out = pd.DataFrame(data,columns=columns)\\\n",
    "        .iloc[:,:-1]\\\n",
    "        .assign(Nation= lambda x: x['Nation'].str.split(' ').str[1], \n",
    "            Pos = lambda x: pd.Categorical(x['Pos']),\n",
    "            Squad = lambda x: pd.Categorical(x['Squad']),\n",
    "            Year = period[0:4])\n",
    "            # Nation = lambda x: pd.Categorical(x['Nation']))\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(periods):\n",
    "    df_list = []\n",
    "    id = 'stats_standard'\n",
    "    for period in periods:\n",
    "        time.sleep(10)\n",
    "        url = f'https://fbref.com/en/comps/9/{period}/stats/{period}-Premier-League-Stats'\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install()) #driver\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        cookie = driver.find_element(By.CLASS_NAME,'qc-cmp2-summary-buttons').click()\n",
    "\n",
    "        columns, data = scrape(driver,id)\n",
    "\n",
    "        df_list.append(clean(columns, data, period))\n",
    "\n",
    "    main_df = pd.concat(df_list)\n",
    "    print(f'Number of observations in main dataset is: {len(main_df)}')\n",
    "    return main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-6fea03c6f6aa>:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install()) #driver\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "51\n",
      "77\n",
      "103\n",
      "129\n",
      "155\n",
      "181\n",
      "207\n",
      "233\n",
      "259\n",
      "285\n",
      "311\n",
      "337\n",
      "363\n",
      "389\n",
      "415\n",
      "441\n",
      "467\n",
      "493\n",
      "519\n",
      "25\n",
      "51\n",
      "77\n",
      "103\n",
      "129\n",
      "155\n",
      "181\n",
      "207\n",
      "233\n",
      "259\n",
      "285\n",
      "311\n",
      "337\n",
      "363\n",
      "389\n",
      "415\n",
      "441\n",
      "467\n",
      "493\n",
      "519\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "51\n",
      "77\n",
      "103\n",
      "129\n",
      "155\n",
      "181\n",
      "207\n",
      "233\n",
      "259\n",
      "285\n",
      "311\n",
      "337\n",
      "363\n",
      "389\n",
      "415\n",
      "441\n",
      "467\n",
      "493\n",
      "519\n",
      "545\n",
      "Number of observations in main dataset is: 2108\n"
     ]
    }
   ],
   "source": [
    "years = ['2018-2019','2019-2020','2020-2021','2021-2022']\n",
    "main_df = get_data(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv('main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(driver):\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    table_node = soup.find(id='stats_standard')\n",
    "\n",
    "    columns_html = table_node.thead.find_all('th')\n",
    "        # Extract the text\n",
    "    columns = []\n",
    "    for col in columns_html:\n",
    "        columns.append(col.text)\n",
    "    columns = columns[7:]\n",
    "\n",
    "    rows_list = table_node.tbody.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    fejl = []\n",
    "    for row_node in rows_list:\n",
    "        # if (rows_list.index(row_node) % 25 == 0):\n",
    "        try:\n",
    "            row = []\n",
    "            for child in row_node.children:\n",
    "                row.append(child.text)\n",
    "            data.append(row)\n",
    "        except:\n",
    "            print(rows_list.index(row_node))\n",
    "            fejl.append(rows_list.index(row_node)) # Column appear every 25th row\n",
    "    \n",
    "    return columns, data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68e77a2019630c844ad2ed089cfeaa1ff41081e574fef0e7734cace63a14be5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
